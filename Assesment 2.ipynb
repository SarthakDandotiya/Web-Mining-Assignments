{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarthak Dandotiya **17BCE0203**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assesment 2\n",
    "\n",
    "### Question 1\n",
    "Write a program to show the implementation of breadth-first and depth-first crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------DFS------------------\n",
      "\n",
      "\n",
      "[887] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica\n",
      "[701] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica#cite_note-102\n",
      "Error\n",
      "[0] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica#cite_note-102#cite_note-69\n",
      "[701] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica#cite_note-102/wiki/Cambridge_University\n",
      "[701] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica#cite_note-102/wiki/Cambridge_University/wiki/UK_Independence_Party\n",
      "[701] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica#cite_note-102/wiki/Cambridge_University/wiki/UK_Independence_Party/w/index.php?title=Cambridge_Analytica&action=edit&section=15\n",
      "Error\n",
      "[0] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica#cite_note-102/wiki/Cambridge_University/wiki/UK_Independence_Party/w/index.php?title=Cambridge_Analytica&action=edit&section=15#cite_note-Rosenburg_M-36\n",
      "[701] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica#cite_note-102/wiki/Cambridge_University/wiki/UK_Independence_Party/w/index.php?title=Cambridge_Analytica&action=edit&section=15//www.worldcat.org/issn/0190-8286\n",
      "[701] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica#cite_note-102/wiki/Cambridge_University/wiki/UK_Independence_Party/w/index.php?title=Cambridge_Analytica&action=edit&section=15//www.worldcat.org/issn/0190-8286/wiki/PubMed_Central\n",
      "Error\n",
      "[0] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica#cite_note-102/wiki/Cambridge_University/wiki/UK_Independence_Party/w/index.php?title=Cambridge_Analytica&action=edit&section=15//www.worldcat.org/issn/0190-8286/wiki/PubMed_Central#cite_ref-issenberg_7-4\n",
      "\n",
      "\n",
      " --- 5093 ---\n",
      "\n",
      "\n",
      "--------------BFS------------------\n",
      "\n",
      "\n",
      "[887] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica\n",
      "[40] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica/w/index.php?title=Cambridge_Analytica&action=edit&section=6\n",
      "Error\n",
      "[0] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica/wiki/Simon_Gicharu\n",
      "[701] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica#cite_ref-131\n",
      "[701] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica#cite_ref-99\n",
      "[701] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica#cite_ref-guardian_51-1\n",
      "Error\n",
      "[0] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica/wiki/Psy-Group\n",
      "Error\n",
      "[0] links found in https://en.wikipedia.org/wiki/Cambridge_Analytica/wiki/Psychographic\n",
      "[326] links found in http://www.prnewswire.com/news-releases/cambridge-analytica-congratulates-senator-ted-cruz-on-iowa-caucus-win-300213608.html\n",
      "[166] links found in https://pt.wikipedia.org/wiki/Cambridge_Analytica\n",
      "\n",
      "\n",
      " --- 3522 ---\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd\n",
    "\n",
    "class Crawler():\n",
    "\n",
    "    def __init__(self, url, branching_factor):\n",
    "        self.url = url\n",
    "        self.branching_factor = branching_factor\n",
    "        self.DFS = 'dfs'\n",
    "        self.BFS = 'bfs'\n",
    "\n",
    "    def get_url(self, current_url, urls_found):\n",
    "        urls = set([])\n",
    "        try:\n",
    "            response = urllib.request.urlopen(current_url)\n",
    "        except:\n",
    "            print('Error')\n",
    "            return urls\n",
    "        soup = BeautifulSoup(response.read())\n",
    "        newLink = soup.findAll('a')\n",
    "        for link in newLink:\n",
    "            try:\n",
    "                link = str(link.get('href'))\n",
    "                m = re.match(r'http+', link)\n",
    "                if not m:\n",
    "                    link = current_url + link\n",
    "                if link not in urls_found:\n",
    "                    urls.add(str(link))\n",
    "            except UnicodeEncodeError as e:\n",
    "                print ('Error ---> ', e)\n",
    "        return urls\n",
    "\n",
    "    def start_dfs(self):\n",
    "        return self.start(self.DFS)\n",
    "\n",
    "    def start_bfs(self):\n",
    "        return self.start(self.BFS)\n",
    "\n",
    "    def start(self, algorithm):\n",
    "        structure = []\n",
    "        urls_found = set([])\n",
    "        structure.append(self.url)\n",
    "        count = 0\n",
    "        while count < self.branching_factor and structure:\n",
    "            if algorithm == self.BFS:\n",
    "                current_url = structure.pop(0)\n",
    "            else:\n",
    "                current_url = structure.pop()\n",
    "            urls = self.get_url(current_url, urls_found)\n",
    "            print(('[%s] links found in %s') %(len(urls), current_url))\n",
    "            for i in urls:\n",
    "                structure.append(i)\n",
    "                urls_found.add(i)\n",
    "            count += 1\n",
    "        print(\"\\n\\n\",\"---\",len(urls_found),\"---\")\n",
    "        return urls_found\n",
    "\n",
    "    def __str__(self):\n",
    "        return ('url:%s branching_factor:%s') %(self.url, self.branching_factor)\n",
    "\n",
    "crawl = Crawler('https://en.wikipedia.org/wiki/Cambridge_Analytica',10)\n",
    "print(\"\\n\\n--------------DFS------------------\\n\\n\")\n",
    "dfs = crawl.start_dfs()\n",
    "print(\"\\n\\n--------------BFS------------------\\n\\n\")\n",
    "bfs = crawl.start_bfs()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Write a program to crawl two different websites, extract the content and save them in\n",
    "separate text documents. Based on the contents of those two documents, prepare\n",
    "one inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>web</th>\n",
       "      <th>mining</th>\n",
       "      <th>is</th>\n",
       "      <th>the</th>\n",
       "      <th>application</th>\n",
       "      <th>of</th>\n",
       "      <th>data</th>\n",
       "      <th>techniques</th>\n",
       "      <th>to</th>\n",
       "      <th>discover</th>\n",
       "      <th>...</th>\n",
       "      <th>analysis</th>\n",
       "      <th>through</th>\n",
       "      <th>unsupervised</th>\n",
       "      <th>its</th>\n",
       "      <th>across</th>\n",
       "      <th>business</th>\n",
       "      <th>problems</th>\n",
       "      <th>referred</th>\n",
       "      <th>predictive</th>\n",
       "      <th>analytics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ML.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;1 [137]&gt;</td>\n",
       "      <td>&lt;6 [3, 32, 90, 106, 138, 163]&gt;</td>\n",
       "      <td>&lt;5 [4, 68, 102, 119, 131]&gt;</td>\n",
       "      <td>&lt;2 [128, 157]&gt;</td>\n",
       "      <td>&lt;6 [7, 37, 79, 121, 133, 141]&gt;</td>\n",
       "      <td>&lt;4 [50, 54, 136, 150]&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;7 [16, 57, 66, 94, 109, 130, 166]&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;1 [151]&gt;</td>\n",
       "      <td>&lt;1 [152]&gt;</td>\n",
       "      <td>&lt;1 [153]&gt;</td>\n",
       "      <td>&lt;1 [156]&gt;</td>\n",
       "      <td>&lt;1 [158]&gt;</td>\n",
       "      <td>&lt;1 [159]&gt;</td>\n",
       "      <td>&lt;1 [160]&gt;</td>\n",
       "      <td>&lt;1 [165]&gt;</td>\n",
       "      <td>&lt;1 [168]&gt;</td>\n",
       "      <td>&lt;1 [169]&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WM.txt</th>\n",
       "      <td>&lt;17 [0, 16, 28, 43, 74, 84, 87, 90, 102, 126, ...</td>\n",
       "      <td>&lt;10 [1, 7, 26, 76, 92, 104, 128, 150, 167, 200]&gt;</td>\n",
       "      <td>&lt;3 [2, 22, 77]&gt;</td>\n",
       "      <td>&lt;17 [3, 13, 18, 27, 71, 83, 96, 108, 112, 115,...</td>\n",
       "      <td>&lt;1 [4]&gt;</td>\n",
       "      <td>&lt;10 [5, 32, 73, 98, 111, 123, 159, 165, 176, 1...</td>\n",
       "      <td>&lt;2 [6, 39]&gt;</td>\n",
       "      <td>&lt;2 [8, 205]&gt;</td>\n",
       "      <td>&lt;8 [9, 35, 49, 51, 78, 106, 171, 190]&gt;</td>\n",
       "      <td>&lt;2 [10, 107]&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 183 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      web  \\\n",
       "ML.txt                                                  0   \n",
       "WM.txt  <17 [0, 16, 28, 43, 74, 84, 87, 90, 102, 126, ...   \n",
       "\n",
       "                                                  mining  \\\n",
       "ML.txt                                         <1 [137]>   \n",
       "WM.txt  <10 [1, 7, 26, 76, 92, 104, 128, 150, 167, 200]>   \n",
       "\n",
       "                                    is  \\\n",
       "ML.txt  <6 [3, 32, 90, 106, 138, 163]>   \n",
       "WM.txt                 <3 [2, 22, 77]>   \n",
       "\n",
       "                                                      the     application  \\\n",
       "ML.txt                         <5 [4, 68, 102, 119, 131]>  <2 [128, 157]>   \n",
       "WM.txt  <17 [3, 13, 18, 27, 71, 83, 96, 108, 112, 115,...         <1 [4]>   \n",
       "\n",
       "                                                       of  \\\n",
       "ML.txt                     <6 [7, 37, 79, 121, 133, 141]>   \n",
       "WM.txt  <10 [5, 32, 73, 98, 111, 123, 159, 165, 176, 1...   \n",
       "\n",
       "                          data    techniques  \\\n",
       "ML.txt  <4 [50, 54, 136, 150]>             0   \n",
       "WM.txt             <2 [6, 39]>  <2 [8, 205]>   \n",
       "\n",
       "                                            to       discover  ...   analysis  \\\n",
       "ML.txt     <7 [16, 57, 66, 94, 109, 130, 166]>              0  ...  <1 [151]>   \n",
       "WM.txt  <8 [9, 35, 49, 51, 78, 106, 171, 190]>  <2 [10, 107]>  ...          0   \n",
       "\n",
       "          through unsupervised        its     across   business   problems  \\\n",
       "ML.txt  <1 [152]>    <1 [153]>  <1 [156]>  <1 [158]>  <1 [159]>  <1 [160]>   \n",
       "WM.txt          0            0          0          0          0          0   \n",
       "\n",
       "         referred predictive  analytics  \n",
       "ML.txt  <1 [165]>  <1 [168]>  <1 [169]>  \n",
       "WM.txt          0          0          0  \n",
       "\n",
       "[2 rows x 183 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from os import system, name\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def save(url):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content)\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    text = soup.get_text()\n",
    "    cleanString1 = re.sub('\\W+',' ', text )\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\" \"))\n",
    "    modifiied_chunks = remove_stopwords(chunks)\n",
    "    text = '\\n'.join(chunk for chunk in modifiied_chunks if chunk)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    my_stopwords = my_stopwords = set(stopwords.words('english'))\n",
    "    my_stopwords.update([\".\", \",\", \"'\", \":\", \";\", \"\\\"\", \"...\",\\\n",
    "                         \"(\", \")\", \"{\", \"}\", \"[\", \"]\", \"©\", \"@\",\\\n",
    "                         \"'m\", \"--\", \"__\", \"?\", \"|\", \"#\", \"!\",\\\n",
    "                         \"-\", \"$\", \"'s\", \"&\", \"·\", \"–\", \"’\", \"‘\",\\\n",
    "                         \"“\", \"”\", \"%\", \"''\", \"+\", \"/\",\"-\",])\n",
    "\n",
    "    filtered1 = [w for w in words if not w in my_stopwords]\n",
    "    return filtered1\n",
    "\n",
    "def cleaning_data(words):\n",
    "    removed_links = re.sub('^\\( & \\)&', ' ',words)\n",
    "    return removed_links\n",
    "\n",
    "def process_files(filenames):\n",
    "    file_to_terms = {}\n",
    "    for file in filenames:\n",
    "        pattern = re.compile('[\\W_]+')\n",
    "        file_to_terms[file] = open(file, 'r',encoding='utf-8').read().lower();\n",
    "        file_to_terms[file] = pattern.sub(' ',file_to_terms[file])\n",
    "        re.sub(r'[\\W_]+','', file_to_terms[file])\n",
    "        file_to_terms[file] = file_to_terms[file].split()\n",
    "    return file_to_terms\n",
    "\n",
    "\n",
    "def index_one_file(termlist):\n",
    "    fileIndex = {}\n",
    "    for index, word in enumerate(termlist):\n",
    "        if word in fileIndex.keys():\n",
    "            fileIndex[word].append(index)\n",
    "        else:\n",
    "            fileIndex[word] = [index]\n",
    "    return fileIndex\n",
    "\n",
    "def make_indices(termlists):\n",
    "    total = {}\n",
    "    for filename in termlists.keys():\n",
    "        total[filename] = index_one_file(termlists[filename])\n",
    "    return total\n",
    "\n",
    "\n",
    "def fullIndex(regdex):\n",
    "    total_index = {}\n",
    "    for filename in regdex.keys():\n",
    "        for word in regdex[filename].keys():\n",
    "            \n",
    "            if word in total_index.keys():\n",
    "                if filename in total_index[word].keys():\n",
    "                    total_index[word][filename].extend(regdex[filename][word][:])\n",
    "                else:\n",
    "                    total_index[word][filename] = regdex[filename][word]\n",
    "            else:\n",
    "                total_index[word] = {filename: regdex[filename][word]}\n",
    "    return total_index\n",
    "\n",
    "def one_word_query(word, invertedIndex):\n",
    "    pattern = re.compile('[\\W_]+')\n",
    "    word = pattern.sub(' ',word)\n",
    "    if word in invertedIndex.keys():\n",
    "        return [filename for filename in invertedIndex[word].values()]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    \n",
    "def free_text_query(string,index):\n",
    "    pattern = re.compile('[\\W_]+')\n",
    "    string = pattern.sub(' ',string)\n",
    "    result = []\n",
    "    for word in string.split():\n",
    "        result += one_word_query(word,index)\n",
    "    return list(set(result))\n",
    "\n",
    "\n",
    "def phrase_query(string, invertedIndex):\n",
    "    pattern = re.compile('[\\W_]+')\n",
    "    string = pattern.sub(' ',string)\n",
    "    listOfLists, result = [],[]\n",
    "    for word in string.split():\n",
    "        listOfLists.append(free_text_query(word,invertedIndex))\n",
    "    setted = set(listOfLists[0]).intersection(*listOfLists)\n",
    "    for filename in setted:\n",
    "        temp = []\n",
    "        for word in string.split():\n",
    "            temp.append(invertedIndex[word][filename][:])\n",
    "        for i in range(len(temp)):\n",
    "            for ind in range(len(temp[i])):\n",
    "                temp[i][ind] -= i\n",
    "        if set(temp[0]).intersection(*temp):\n",
    "            result.append(filename)\n",
    "        print('\\n temp : \\n')\n",
    "        print(temp)\n",
    "    return result\n",
    "\n",
    "urls = [\"https://en.wikipedia.org/wiki/Web_mining\", \"https://en.wikipedia.org/wiki/Machine_learning\"]\n",
    "\n",
    "for url in urls:\n",
    "    text = save(url)\n",
    "    with open(str(url.split(\"/\")[-1])+'.txt','w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "filenames = [\"WM.txt\", \"ML.txt\"]\n",
    "\n",
    "# filenames = []\n",
    "# for i in os.listdir():\n",
    "#     if i.endswith('.txt'):\n",
    "#         filenames.append(i)\n",
    "\n",
    "termslist = process_files(filenames)\n",
    "\n",
    "totaldict = make_indices(termslist)\n",
    "\n",
    "index = fullIndex(totaldict)\n",
    "\n",
    "inv_matrix = pd.DataFrame(index)\n",
    "inv_matrix = inv_matrix.T\n",
    "inv_matrix.rename(columns = {\"Machine_learning.txt\":\"Document1\", \"Web_mining.txt\":\"Document2\"},inplace=True)\n",
    "\n",
    "def check_nan(val):\n",
    "    if isinstance(val, float):\n",
    "        return 0\n",
    "    elif isinstance(val, list):\n",
    "        return ('<' + str(len(val)) + ' ' +  str(val) + '>') \n",
    "    \n",
    "inv_matrix = inv_matrix.applymap(check_nan)\n",
    "display(inv_matrix.T)\n",
    "\n",
    "inv_matrix.T.to_csv(r'inv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Write a program to find out rank of each documents prepared in question number 2\n",
    "with respect to a Query using TF-IDF. (Show the rank of each document based on\n",
    "both Cosine similarity and Euclidean Distance.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accessing</th>\n",
       "      <th>across</th>\n",
       "      <th>activities</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>algorithms</th>\n",
       "      <th>also</th>\n",
       "      <th>analysis</th>\n",
       "      <th>analytics</th>\n",
       "      <th>another</th>\n",
       "      <th>apparatuses</th>\n",
       "      <th>...</th>\n",
       "      <th>vision</th>\n",
       "      <th>web</th>\n",
       "      <th>website</th>\n",
       "      <th>wide</th>\n",
       "      <th>within</th>\n",
       "      <th>without</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>EUCLIDIAN</th>\n",
       "      <th>COSINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00608</td>\n",
       "      <td>0.018241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00608</td>\n",
       "      <td>0.00608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00608</td>\n",
       "      <td>0.01216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241437</td>\n",
       "      <td>0.013486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.086011</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.258699</td>\n",
       "      <td>0.011924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      accessing   across  activities  algorithm  algorithms  also  analysis  \\\n",
       "Doc1   0.000000  0.00608    0.000000    0.00608    0.018241   0.0   0.00608   \n",
       "Doc2   0.005059  0.00000    0.005059    0.00000    0.000000   0.0   0.00000   \n",
       "\n",
       "      analytics   another  apparatuses  ...   vision       web   website  \\\n",
       "Doc1    0.00608  0.000000     0.000000  ...  0.00608  0.000000  0.000000   \n",
       "Doc2    0.00000  0.005059     0.005059  ...  0.00000  0.086011  0.005059   \n",
       "\n",
       "      wide   within  without     world     would  EUCLIDIAN    COSINE  \n",
       "Doc1   0.0  0.00608  0.01216  0.000000  0.000000   0.241437  0.013486  \n",
       "Doc2   0.0  0.00000  0.00000  0.005059  0.005059   0.258699  0.011924  \n",
       "\n",
       "[2 rows x 156 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Ranking:\n",
      "1 -> Doc1\n",
      "2 -> Doc2\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def stopw(f):\n",
    "    stoplist = stopwords.words('english')\n",
    "    file = open(f,'r',encoding='utf-8')\n",
    "    text = file.read()\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words=set(stop_words)\n",
    "    stop_words.update([\".\", \",\", \"'\", \":\", \";\", \"\\\"\", \"...\",\\\n",
    "                         \"(\", \")\", \"{\", \"}\", \"[\", \"]\", \"©\", \"@\",\\\n",
    "                         \"'m\", \"--\", \"__\", \"?\", \"|\", \"#\", \"!\",\\\n",
    "                         \"-\", \"$\", \"'s\", \"&\", \"·\", \"–\", \"’\", \"‘\",\\\n",
    "                         \"“\", \"”\", \"%\", \"''\", \"+\", \"/\",\"-\",\"``\"])\n",
    "    word_tokens = word_tokenize(text)\n",
    "    num_of_lines = sum(1 for line in file)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w.lower())\n",
    "    file.close\n",
    "    return filtered_sentence\n",
    "\n",
    "filtered_sentencea= stopw(\"ML.txt\")\n",
    "filtered_sentenceb= stopw('WM.txt')\n",
    "\n",
    "wordSet = set(filtered_sentencea).union(set(filtered_sentenceb))\n",
    "\n",
    "wordDictA = dict.fromkeys(wordSet, 0)\n",
    "wordDictB = dict.fromkeys(wordSet, 0)\n",
    "\n",
    "\n",
    "for word in filtered_sentencea:\n",
    "    wordDictA[word]+= 1\n",
    "    \n",
    "for word in filtered_sentenceb:\n",
    "    wordDictB[word]+=1\n",
    "\n",
    "    \n",
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict\n",
    "\n",
    "tfA = computeTF(wordDictA, filtered_sentencea)\n",
    "tfB = computeTF(wordDictB, filtered_sentenceb)\n",
    "\n",
    "\n",
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "        \n",
    "    return idfDict\n",
    "\n",
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "\n",
    "\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "\n",
    "df = pd.DataFrame([tfidfA, tfidfB],)\n",
    "df.head()\n",
    "\n",
    "# print(\"TFIDF:\")\n",
    "\n",
    "df.rename(index={0:'Doc1',1:'Doc2'}, inplace=True)\n",
    "# display(df.head())\n",
    "\n",
    "\n",
    "def process_query(query):\n",
    "    query_arr = query.split(\" \")\n",
    "    qur_arr = {word:query_arr.count(word)/len(query_arr) for word in query_arr}\n",
    "    modified_arr = {}\n",
    "    for i in df.columns:\n",
    "        if i in list(qur_arr.keys()):\n",
    "            modified_arr[i] = qur_arr[i]\n",
    "        else:\n",
    "            modified_arr[i] = 0\n",
    "  \n",
    "    return modified_arr\n",
    "    \n",
    "def calculate_euclidean_Dist(v1,v2):\n",
    "    v1 = v1.astype('float64')\n",
    "    mult_resul = v1.sub(v2,fill_value=0)**2\n",
    "    return sum(list(mult_resul))**0.5\n",
    "    \n",
    "def calculate_cosine(v1,v2):\n",
    "    v1 = v1.astype('float64')\n",
    "    sum_res = v1.mul(v2,fill_value=0)\n",
    "    return sum(list(sum_res))\n",
    "\n",
    "query = \"web mining in machine learning\"\n",
    "# query = \"web mining\"\n",
    "# query = \"machine learning\"\n",
    "tf_query = process_query(query)\n",
    "tf_idf_query = computeTFIDF(tf_query,idfs)\n",
    "# print(\"For Query- \",tf_idf_query)\n",
    "df['EUCLIDIAN'] = 0\n",
    "df['COSINE'] = 0\n",
    "for i in range(len(df.index)):\n",
    "    df.iloc[i,-2] = calculate_euclidean_Dist(df.loc[list(df.index)[i],:], pd.Series(tf_idf_query))\n",
    "    df.iloc[i,-1] = calculate_cosine(df.loc[list(df.index)[i],:], pd.Series(tf_idf_query))\n",
    "\n",
    "print(\"TFIDF:\")\n",
    "display(df.head())\n",
    "\n",
    "def rank_docs(cosine_sr,euc_sr):\n",
    "    cosin_sr_list = cosine_sr.sort_values(ascending=False)\n",
    "    euc_sr_list = euc_sr.sort_values()\n",
    "    for i in range(len(euc_sr_list)):\n",
    "        if list(cosin_sr_list.index)[i] == list(cosin_sr_list.index)[i]:\n",
    "            print(str(i+1) + ' -> '+ list(cosin_sr_list.index)[i])\n",
    "\n",
    "print(\"Document Ranking:\")\n",
    "rank_docs(df.iloc[:,-1],df.iloc[:,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
